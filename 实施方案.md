# OCR文档智能处理系统 - 详细实施方案

## 一、项目概述

### 1.1 项目目标
构建一个基于LangGraph的智能OCR文档处理系统，能够自动识别上传的文档图像/PDF，进行文档分类（快递单、抽样单、测试单），提取结构化信息，并通过FastAPI提供RESTful API接口，使用Supabase作为数据存储后端，用户管理。

### 1.2 核心功能
1. **文档上传与存储**
2. **OCR文本提取**（基于PaddleOCR）
3. **智能文档分类**（使用LangGraph+LLM）
4. **结构化信息提取**（根据不同文档类型）
5. **RESTful API服务**（FastAPI）
6. **后端服务**（Supabase）

## 二、技术架构

### 2.1 架构图
```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│  客户端         │    │  FastAPI网关    │    │   LangGraph     │
│  (Web/移动端)   │───▶│   Rest API      │───▶│   Agentic流程   │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                            │                         │
                    ┌───────┴───────┐         ┌───────┴───────┐
                    │  文件存储      │         │  LLM服务      │
                    │  (本地/云)    │         │  (OpenAI)     │
                    └───────┬───────┘         └───────┬───────┘
                            │                         │
                    ┌───────┴───────┐         ┌───────┴───────┐
                    │  OCR服务      │         │  提取逻辑     │
                    │  (PaddleOCR)  │         │  (Prompt)     │
                    └───────┬───────┘         └───────┬───────┘
                            │                         │
                    ┌───────┴───────┐         ┌───────┴───────┐
                    │  Supabase     │◀────────│  结果存储     │
                    │  数据库       │         │               │
                    └───────────────┘         └───────────────┘
```

### 2.2 技术栈
- **后端框架**: FastAPI + Uvicorn
- **智能体框架**: LangGraph 
- **OCR引擎**: PaddleOCR
- **数据库**: Supabase (PostgreSQL)
- **LLM服务**: OpenAI API / DeepSeek API
- **部署**: Docker + Docker Compose
- **文件存储**: 本地文件系统 / S3兼容存储

## 三、详细实施步骤

### 3.1 环境准备

#### 3.1.1 Python环境
```bash
# 创建虚拟环境
python -m venv venv

# 激活虚拟环境
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# 安装依赖
pip install -r requirements.txt
```

#### 3.1.2 Supabase设置
1. 访问 https://supabase.com 创建新项目
2. 获取项目URL和API密钥
3. 执行SQL初始化脚本（见上文）
4. 配置环境变量

#### 3.1.3 OCR模型
```bash
model/
```

### 3.2 数据库设计

#### 3.2.1 Supabase表结构

```sql
-- 1. 文档表 (documents)
-- OCR文档处理系统 - 简化版数据库部署脚本
-- 1. 创建扩展
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- 2. 创建表
-- 文档主表
CREATE TABLE documents (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    user_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
    file_name VARCHAR(500) NOT NULL,
    original_file_name VARCHAR(500),
    file_path VARCHAR(1000) NOT NULL,
    file_size BIGINT,
    file_type VARCHAR(100),
    file_extension VARCHAR(50),
    mime_type VARCHAR(100),
    document_type VARCHAR(50),
    status VARCHAR(50) DEFAULT 'pending' CHECK (status IN (
        'pending', 'uploaded', 'processing', 'completed', 'failed'
    )),
    ocr_text TEXT,
    ocr_confidence FLOAT,
    error_message TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    processed_at TIMESTAMP WITH TIME ZONE
);

-- 检验报告表
CREATE TABLE inspection_reports (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
    sample_name VARCHAR(255),
    specification_model VARCHAR(255),
    production_date_batch VARCHAR(255),
    inspected_unit_name VARCHAR(255),
    inspected_unit_address VARCHAR(512),
    inspected_unit_phone VARCHAR(64),
    manufacturer_name VARCHAR(255),
    manufacturer_address VARCHAR(512),
    manufacturer_phone VARCHAR(64),
    task_source VARCHAR(255),
    sampling_agency VARCHAR(255),
    sampling_date DATE,
    inspection_conclusion VARCHAR(255),
    inspection_category VARCHAR(255),
    notes TEXT,
    inspector VARCHAR(64),
    reviewer VARCHAR(64),
    approver VARCHAR(64),
    extraction_confidence FLOAT,
    extraction_version VARCHAR(50) DEFAULT '1.0',
    raw_extraction_data JSONB,
    is_validated BOOLEAN DEFAULT FALSE,
    validated_by UUID REFERENCES auth.users(id),
    validated_at TIMESTAMP WITH TIME ZONE,
    validation_notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    CONSTRAINT inspection_reports_document_unique UNIQUE(document_id)
);

-- 快递面单表
CREATE TABLE expresses (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
    tracking_number VARCHAR(64),
    recipient VARCHAR(128),
    delivery_address TEXT,
    sender VARCHAR(128),
    sender_address TEXT,
    notes TEXT,
    extraction_confidence FLOAT,
    extraction_version VARCHAR(50) DEFAULT '1.0',
    raw_extraction_data JSONB,
    is_validated BOOLEAN DEFAULT FALSE,
    validated_by UUID REFERENCES auth.users(id),
    validated_at TIMESTAMP WITH TIME ZONE,
    validation_notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    CONSTRAINT expresses_document_unique UNIQUE(document_id)
);

-- 市场抽检表
CREATE TABLE sampling_forms (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
    inspection_result VARCHAR(64),
    product_name VARCHAR(255),
    specification_model VARCHAR(255),
    sampled_entity VARCHAR(255),
    labeled_manufacturer VARCHAR(255),
    market_regulatory_bureau VARCHAR(255),
    sampling_unit VARCHAR(255),
    sampling_date DATE,
    sampled_province VARCHAR(64),
    sampled_city VARCHAR(64),
    extraction_confidence FLOAT,
    extraction_version VARCHAR(50) DEFAULT '1.0',
    raw_extraction_data JSONB,
    is_validated BOOLEAN DEFAULT FALSE,
    validated_by UUID REFERENCES auth.users(id),
    validated_at TIMESTAMP WITH TIME ZONE,
    validation_notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    CONSTRAINT sampling_forms_document_unique UNIQUE(document_id)
);

-- 处理日志表（可选）
CREATE TABLE processing_logs (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
    step VARCHAR(100) NOT NULL,
    status VARCHAR(50) NOT NULL,
    message TEXT,
    error_details TEXT,
    duration_ms INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- 3. 创建索引
CREATE INDEX idx_documents_user_id ON documents(user_id);
CREATE INDEX idx_documents_status ON documents(status);
CREATE INDEX idx_documents_document_type ON documents(document_type);
CREATE INDEX idx_documents_created_at ON documents(created_at DESC);

CREATE INDEX idx_inspection_reports_document_id ON inspection_reports(document_id);
CREATE INDEX idx_inspection_reports_sample_name ON inspection_reports(sample_name);
CREATE INDEX idx_inspection_reports_manufacturer_name ON inspection_reports(manufacturer_name);

CREATE INDEX idx_expresses_document_id ON expresses(document_id);
CREATE INDEX idx_expresses_tracking_number ON expresses(tracking_number);

CREATE INDEX idx_sampling_forms_document_id ON sampling_forms(document_id);
CREATE INDEX idx_sampling_forms_product_name ON sampling_forms(product_name);

CREATE INDEX idx_processing_logs_document_id ON processing_logs(document_id);
CREATE INDEX idx_processing_logs_step ON processing_logs(step);

-- 4. 创建触发器函数
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE OR REPLACE FUNCTION update_document_status()
RETURNS TRIGGER AS $$
BEGIN
    UPDATE documents 
    SET status = 'completed',
        document_type = CASE 
            WHEN TG_TABLE_NAME = 'inspection_reports' THEN 'inspection_report'
            WHEN TG_TABLE_NAME = 'expresses' THEN 'express'
            WHEN TG_TABLE_NAME = 'sampling_forms' THEN 'sampling_form'
        END,
        processed_at = NOW()
    WHERE id = NEW.document_id;
    
    RETURN NEW;
END;
$$ language 'plpgsql';

-- 5. 添加触发器
CREATE TRIGGER update_documents_updated_at 
    BEFORE UPDATE ON documents
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_inspection_reports_updated_at 
    BEFORE UPDATE ON inspection_reports
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_expresses_updated_at 
    BEFORE UPDATE ON expresses
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_sampling_forms_updated_at 
    BEFORE UPDATE ON sampling_forms
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_doc_status_after_inspection_report 
    AFTER INSERT ON inspection_reports
    FOR EACH ROW EXECUTE FUNCTION update_document_status();

CREATE TRIGGER update_doc_status_after_express 
    AFTER INSERT ON expresses
    FOR EACH ROW EXECUTE FUNCTION update_document_status();

CREATE TRIGGER update_doc_status_after_sampling_form 
    AFTER INSERT ON sampling_forms
    FOR EACH ROW EXECUTE FUNCTION update_document_status();

-- 6. 启用RLS
ALTER TABLE documents ENABLE ROW LEVEL SECURITY;
ALTER TABLE inspection_reports ENABLE ROW LEVEL SECURITY;
ALTER TABLE expresses ENABLE ROW LEVEL SECURITY;
ALTER TABLE sampling_forms ENABLE ROW LEVEL SECURITY;
ALTER TABLE processing_logs ENABLE ROW LEVEL SECURITY;

-- 7. 创建RLS策略
CREATE POLICY "用户可以管理自己的文档" ON documents
FOR ALL USING (user_id = auth.uid());

CREATE POLICY "用户可以查看自己文档的检验报告" ON inspection_reports
FOR ALL USING (
    document_id IN (
        SELECT id FROM documents WHERE user_id = auth.uid()
    )
);

CREATE POLICY "用户可以查看自己文档的快递面单" ON expresses
FOR ALL USING (
    document_id IN (
        SELECT id FROM documents WHERE user_id = auth.uid()
    )
);

CREATE POLICY "用户可以查看自己文档的市场抽检表" ON sampling_forms
FOR ALL USING (
    document_id IN (
        SELECT id FROM documents WHERE user_id = auth.uid()
    )
);

CREATE POLICY "用户可以查看自己文档的处理日志" ON processing_logs
FOR ALL USING (
    document_id IN (
        SELECT id FROM documents WHERE user_id = auth.uid()
    )
);

-- 完成
SELECT '数据库部署完成！' as message;
```

### 3.3 核心模块实现

#### 3.3.1 配置文件 (.env)
```bash
# 应用配置
APP_NAME=OCR-Document-Processor
DEBUG=true
HOST=0.0.0.0
PORT=8000

# 安全配置
SECRET_KEY=your-secret-key-here
JWT_ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Supabase配置
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-anon-key
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key

# LLM配置
OPENAI_API_KEY=sk-your-openai-key
OPENAI_BASE_URL=https://api.openai.com/v1
LLM_MODEL=gpt-4o-mini
LLM_TEMPERATURE=0.7

# OCR配置
OCR_DET_MODEL_PATH=./models/det
OCR_REC_MODEL_PATH=./models/rec
OCR_ORI_MODEL_PATH=./models/ori
OCR_DOC_MODEL_PATH=./models/doc

# 文件存储
UPLOAD_FOLDER=./uploads
MAX_FILE_SIZE=10485760  # 10MB
ALLOWED_EXTENSIONS=.pdf,.png,.jpg,.jpeg,.tiff
```

#### 3.3.2 FastAPI主应用

```python
# api/main.py
from fastapi import FastAPI, Depends, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from contextlib import asynccontextmanager
import logging

from api.routes import documents, users, health
from services.supabase_service import SupabaseService
from services.ocr_service import OCRService
from services.cache_service import RedisCache
from config.settings import settings

# 配置日志
logging.basicConfig(
    level=logging.INFO if not settings.DEBUG else logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """应用生命周期管理"""
    # 启动时初始化服务
    logger.info("正在初始化服务...")
    
    # 初始化Supabase
    app.state.supabase = SupabaseService()
    await app.state.supabase.initialize()
    
    # 初始化OCR服务
    app.state.ocr_service = OCRService()
    await app.state.ocr_service.initialize()
    
    # 初始化缓存
    if settings.REDIS_URL:
        app.state.cache = RedisCache()
        await app.state.cache.initialize()
    
    # 创建必要的目录
    import os
    os.makedirs(settings.UPLOAD_FOLDER, exist_ok=True)
    os.makedirs("logs", exist_ok=True)
    
    logger.info("服务初始化完成")
    yield
    
    # 关闭时清理
    logger.info("正在关闭服务...")
    if hasattr(app.state, 'cache'):
        await app.state.cache.close()
    logger.info("服务已关闭")

# 创建FastAPI应用
app = FastAPI(
    title=settings.APP_NAME,
    description="基于LangGraph的OCR文档智能处理系统",
    version="1.0.0",
    docs_url="/api/docs",
    redoc_url="/api/redoc",
    openapi_url="/api/openapi.json",
    lifespan=lifespan
)

# 添加中间件
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(
    TrustedHostMiddleware,
    allowed_hosts=settings.ALLOWED_HOSTS
)

# 注册路由
app.include_router(health.router, prefix="/api", tags=["系统健康"])
app.include_router(documents.router, prefix="/api/documents", tags=["文档处理"])
app.include_router(users.router, prefix="/api/users", tags=["用户管理"])

# 全局异常处理
@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    logger.error(f"HTTP异常: {exc.status_code} - {exc.detail}")
    return JSONResponse(
        status_code=exc.status_code,
        content={"error": exc.detail}
    )

@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    logger.error(f"未处理异常: {str(exc)}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"error": "内部服务器错误"}
    )

# 主页
@app.get("/")
async def root():
    return {
        "app": settings.APP_NAME,
        "version": "1.0.0",
        "status": "running",
        "docs": "/api/docs",
        "health": "/api/health"
    }
```

#### 3.3.3 LangGraph智能体工作流

```python
# agents/ocr_workflow.py
import asyncio
import json
from typing import Dict, Any, List
from datetime import datetime
from langgraph.graph import StateGraph, END
from langgraph.checkpoint import MemorySaver
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

from agents.state import AgentState
from services.ocr_service import OCRService
from services.supabase_service import SupabaseService
from config.prompts import PromptManager

class OCRWorkflow:
    """OCR处理工作流"""
    
    def __init__(self):
        self.ocr_service = OCRService()
        self.supabase = SupabaseService()
        self.prompts = PromptManager()
        self.workflow = self._build_workflow()
        self.checkpointer = MemorySaver()
    
    def _build_workflow(self) -> StateGraph:
        """构建完整的工作流"""
        workflow = StateGraph(AgentState)
        
        # 添加节点
        workflow.add_node("pre_processing", self._pre_processing_node)
        workflow.add_node("ocr_extraction", self._ocr_extraction_node)
        workflow.add_node("document_classification", self._classification_node)
        workflow.add_node("field_extraction", self._extraction_node)
        workflow.add_node("validation", self._validation_node)
        workflow.add_node("result_storage", self._storage_node)
        workflow.add_node("post_processing", self._post_processing_node)
        
        # 定义边（正常流程）
        workflow.add_edge("pre_processing", "ocr_extraction")
        workflow.add_edge("ocr_extraction", "document_classification")
        workflow.add_edge("document_classification", "field_extraction")
        workflow.add_edge("field_extraction", "validation")
        workflow.add_edge("validation", "result_storage")
        workflow.add_edge("result_storage", "post_processing")
        workflow.add_edge("post_processing", END)
        
        # 错误处理边
        workflow.add_conditional_edges(
            "ocr_extraction",
            self._check_ocr_error,
            {
                "success": "document_classification",
                "retry": "ocr_extraction",
                "fail": END
            }
        )
        
        workflow.add_conditional_edges(
            "validation",
            self._check_validation,
            {
                "valid": "result_storage",
                "invalid": "field_extraction",  # 重新提取
                "fail": END
            }
        )
        
        return workflow.compile(checkpointer=self.checkpointer)
    
    async def _pre_processing_node(self, state: AgentState) -> AgentState:
        """预处理节点"""
        try:
            document_id = state["document_id"]
            
            # 记录开始时间
            state["processing_start"] = datetime.now()
            
            # 获取文档信息
            document = await self.supabase.get_document(document_id)
            if not document:
                raise ValueError(f"文档不存在: {document_id}")
            
            # 更新文档状态为处理中
            await self.supabase.update_document_status(
                document_id=document_id,
                status="processing",
                metadata={"start_time": state["processing_start"].isoformat()}
            )
            
            # 记录日志
            await self.supabase.log_processing_step(
                document_id=document_id,
                step="pre_processing",
                status="started",
                message="开始文档预处理"
            )
            
            # 验证文件
            file_path = document["file_path"]
            if not await self._validate_file(file_path):
                raise ValueError("文件验证失败")
            
            return {
                **state,
                "document_info": document,
                "file_path": file_path,
                "step": "pre_processed",
                "messages": [AIMessage(content="预处理完成")]
            }
            
        except Exception as e:
            await self.supabase.log_processing_step(
                document_id=state.get("document_id"),
                step="pre_processing",
                status="failed",
                message=f"预处理失败: {str(e)}"
            )
            raise
    
    async def _ocr_extraction_node(self, state: AgentState) -> AgentState:
        """OCR提取节点"""
        try:
            document_id = state["document_id"]
            file_path = state["file_path"]
            
            await self.supabase.log_processing_step(
                document_id=document_id,
                step="ocr_extraction",
                status="started",
                message="开始OCR文本提取"
            )
            
            # 执行OCR
            ocr_result = await self.ocr_service.process_document(file_path)
            
            # 计算置信度
            confidence = self._calculate_confidence(ocr_result)
            
            await self.supabase.log_processing_step(
                document_id=document_id,
                step="ocr_extraction",
                status="completed",
                message=f"OCR提取完成，置信度: {confidence:.2f}"
            )
            
            return {
                **state,
                "ocr_text": ocr_result,
                "ocr_confidence": confidence,
                "step": "ocr_extracted",
                "messages": [AIMessage(content="OCR提取完成")]
            }
            
        except Exception as e:
            await self.supabase.log_processing_step(
                document_id=state.get("document_id"),
                step="ocr_extraction",
                status="failed",
                message=f"OCR提取失败: {str(e)}"
            )
            state["error"] = str(e)
            return state
    
    async def _classification_node(self, state: AgentState) -> AgentState:
        """文档分类节点"""
        try:
            document_id = state["document_id"]
            ocr_text = state.get("ocr_text", "")
            
            await self.supabase.log_processing_step(
                document_id=document_id,
                step="document_classification",
                status="started",
                message="开始文档分类"
            )
            
            # 使用LLM进行分类
            llm = self._get_llm()
            prompt = self.prompts.get_classification_prompt(ocr_text)
            response = await llm.ainvoke(prompt)
            
            # 解析响应
            try:
                result = json.loads(response.content)
                doc_type = result.get("文档类型", "未知")
            except:
                doc_type = self._fallback_classification(ocr_text)
            
            await self.supabase.log_processing_step(
                document_id=document_id,
                step="document_classification",
                status="completed",
                message=f"文档分类完成: {doc_type}"
            )
            
            # 更新文档类型
            await self.supabase.update_document_type(document_id, doc_type)
            
            return {
                **state,
                "document_type": doc_type,
                "step": "classified",
                "messages": [AIMessage(content=f"文档分类完成: {doc_type}")]
            }
            
        except Exception as e:
            await self.supabase.log_processing_step(
                document_id=state.get("document_id"),
                step="document_classification",
                status="failed",
                message=f"文档分类失败: {str(e)}"
            )
            state["error"] = str(e)
            return state
    
    async def _extraction_node(self, state: AgentState) -> AgentState:
        """字段提取节点"""
        try:
            document_id = state["document_id"]
            ocr_text = state.get("ocr_text", "")
            doc_type = state.get("document_type", "")
            
            await self.supabase.log_processing_step(
                document_id=document_id,
                step="field_extraction",
                status="started",
                message=f"开始字段提取，文档类型: {doc_type}"
            )
            
            # 根据文档类型选择对应的prompt
            if doc_type == "测试单":
                prompt = self.prompts.get_test_report_prompt(ocr_text)
            elif doc_type == "快递单":
                prompt = self.prompts.get_express_prompt(ocr_text)
            elif doc_type == "抽样单":
                prompt = self.prompts.get_sample_prompt(ocr_text)
            else:
                raise ValueError(f"不支持的文档类型: {doc_type}")
            
            # 使用LLM提取字段
            llm = self._get_llm()
            response = await llm.ainvoke(prompt)
            
            # 解析JSON响应
            try:
                extraction_data = json.loads(response.content)
            except json.JSONDecodeError:
                # 尝试清理响应
                extraction_data = self._clean_json_response(response.content)
            
            await self.supabase.log_processing_step(
                document_id=document_id,
                step="field_extraction",
                status="completed",
                message="字段提取完成"
            )
            
            return {
                **state,
                "extraction_data": extraction_data,
                "step": "extracted",
                "messages": [AIMessage(content="字段提取完成")]
            }
            
        except Exception as e:
            await self.supabase.log_processing_step(
                document_id=state.get("document_id"),
                step="field_extraction",
                status="failed",
                message=f"字段提取失败: {str(e)}"
            )
            state["error"] = str(e)
            return state
    
    async def _validation_node(self, state: AgentState) -> AgentState:
        """验证节点"""
        try:
            document_id = state["document_id"]
            extraction_data = state.get("extraction_data", {})
            doc_type = state.get("document_type", "")
            
            await self.supabase.log_processing_step(
                document_id=document_id,
                step="validation",
                status="started",
                message="开始验证提取结果"
            )
            
            # 验证逻辑
            is_valid, validation_notes = await self._validate_extraction(
                extraction_data, doc_type
            )
            
            validation_status = "valid" if is_valid else "invalid"
            
            await self.supabase.log_processing_step(
                document_id=document_id,
                step="validation",
                status=validation_status,
                message=f"验证完成: {validation_notes}"
            )
            
            return {
                **state,
                "validation_status": validation_status,
                "validation_notes": validation_notes,
                "step": "validated",
                "messages": [AIMessage(content=f"验证完成: {validation_status}")]
            }
            
        except Exception as e:
            await self.supabase.log_processing_step(
                document_id=state.get("document_id"),
                step="validation",
                status="failed",
                message=f"验证失败: {str(e)}"
            )
            state["error"] = str(e)
            return state
    
    async def _storage_node(self, state: AgentState) -> AgentState:
        """存储节点"""
        try:
            document_id = state["document_id"]
            extraction_data = state.get("extraction_data", {})
            doc_type = state.get("document_type", "")
            ocr_text = state.get("ocr_text", "")
            
            await self.supabase.log_processing_step(
                document_id=document_id,
                step="result_storage",
                status="started",
                message="开始保存提取结果"
            )
            
            # 计算处理时间
            processing_end = datetime.now()
            processing_start = state.get("processing_start", processing_end)
            processing_time = int((processing_end - processing_start).total_seconds())
            
            # 保存到Supabase
            extraction_result = {
                "document_id": document_id,
                "document_type": doc_type,
                "extraction_data": extraction_data,
                "raw_ocr_text": ocr_text,
                "confidence_score": state.get("ocr_confidence", 0.0),
                "processing_time": processing_time,
                "validation_status": state.get("validation_status", "pending"),
                "validation_notes": state.get("validation_notes", "")
            }
            
            await self.supabase.save_extraction_result(extraction_result)
            
            # 更新文档状态
            await self.supabase.update_document_status(
                document_id=document_id,
                status="completed",
                metadata={
                    "end_time": processing_end.isoformat(),
                    "processing_time": processing_time,
                    "document_type": doc_type
                }
            )
            
            await self.supabase.log_processing_step(
                document_id=document_id,
                step="result_storage",
                status="completed",
                message="结果保存完成"
            )
            
            return {
                **state,
                "step": "stored",
                "processing_time": processing_time,
                "messages": [AIMessage(content="结果保存完成")]
            }
            
        except Exception as e:
            await self.supabase.log_processing_step(
                document_id=state.get("document_id"),
                step="result_storage",
                status="failed",
                message=f"结果保存失败: {str(e)}"
            )
            
            # 更新文档状态为失败
            await self.supabase.update_document_status(
                document_id=state.get("document_id"),
                status="failed",
                error_message=str(e)
            )
            
            state["error"] = str(e)
            return state
    
    async def _post_processing_node(self, state: AgentState) -> AgentState:
        """后处理节点"""
        try:
            document_id = state["document_id"]
            
            await self.supabase.log_processing_step(
                document_id=document_id,
                step="post_processing",
                status="started",
                message="开始后处理"
            )
            
            # 清理临时文件
            if state.get("file_path"):
                await self._cleanup_temp_files(state["file_path"])
            
            # 发送通知（可选）
            if state.get("user_id"):
                await self._send_notification(
                    user_id=state["user_id"],
                    document_id=document_id,
                    status="completed"
                )
            
            await self.supabase.log_processing_step(
                document_id=document_id,
                step="post_processing",
                status="completed",
                message="后处理完成"
            )
            
            return {
                **state,
                "step": "completed",
                "messages": [AIMessage(content="所有处理步骤完成")]
            }
            
        except Exception as e:
            await self.supabase.log_processing_step(
                document_id=state.get("document_id"),
                step="post_processing",
                status="failed",
                message=f"后处理失败: {str(e)}"
            )
            # 不抛出异常，因为主要处理已完成
            return state
    
    # 辅助方法
    def _check_ocr_error(self, state: AgentState) -> str:
        """检查OCR错误"""
        if state.get("error"):
            retry_count = state.get("ocr_retry_count", 0)
            if retry_count < 3:
                state["ocr_retry_count"] = retry_count + 1
                return "retry"
            return "fail"
        return "success"
    
    def _check_validation(self, state: AgentState) -> str:
        """检查验证结果"""
        validation_status = state.get("validation_status", "pending")
        retry_count = state.get("validation_retry_count", 0)
        
        if validation_status == "valid":
            return "valid"
        elif validation_status == "invalid" and retry_count < 2:
            state["validation_retry_count"] = retry_count + 1
            return "invalid"
        else:
            return "fail"
    
    async def process_document(self, document_id: str, user_id: str = None) -> Dict[str, Any]:
        """处理文档的主入口"""
        initial_state = AgentState(
            document_id=document_id,
            user_id=user_id,
            step="start",
            messages=[],
            ocr_text="",
            document_type="",
            extraction_data={},
            processing_start=datetime.now()
        )
        
        config = {"configurable": {"thread_id": document_id}}
        
        try:
            # 执行工作流
            final_state = await self.workflow.ainvoke(initial_state, config=config)
            
            return {
                "success": "error" not in final_state,
                "document_id": document_id,
                "document_type": final_state.get("document_type"),
                "processing_time": final_state.get("processing_time"),
                "validation_status": final_state.get("validation_status"),
                "error": final_state.get("error"),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"工作流执行失败: {str(e)}", exc_info=True)
            return {
                "success": False,
                "document_id": document_id,
                "error": str(e)
            }
```

#### 3.3.4 OCR服务封装

```python
# services/ocr_service.py
import asyncio
import concurrent.futures
from typing import List, Dict, Any
from paddleocr import PaddleOCR
import os

from config.settings import settings

class OCRService:
    """OCR服务封装"""
    
    def __init__(self):
        self.ocr_engine = None
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)
    
    async def initialize(self):
        """初始化OCR引擎"""
        try:
            # 检查模型路径
            model_paths = [
                settings.OCR_DET_MODEL_PATH,
                settings.OCR_REC_MODEL_PATH,
                settings.OCR_ORI_MODEL_PATH,
                settings.OCR_DOC_MODEL_PATH
            ]
            
            for path in model_paths:
                if not os.path.exists(path):
                    raise FileNotFoundError(f"模型路径不存在: {path}")
            
            # 异步初始化OCR引擎
            self.ocr_engine = await self._init_ocr_engine()
            return True
            
        except Exception as e:
            logger.error(f"OCR引擎初始化失败: {str(e)}")
            return False
    
    async def _init_ocr_engine(self):
        """初始化PaddleOCR引擎"""
        loop = asyncio.get_event_loop()
        
        def init_sync():
            return PaddleOCR(
                lang='ch',
                det_model_dir=settings.OCR_DET_MODEL_PATH,
                rec_model_dir=settings.OCR_REC_MODEL_PATH,
                textline_orientation_model_dir=settings.OCR_ORI_MODEL_PATH,
                doc_orientation_classify_model_dir=settings.OCR_DOC_MODEL_PATH,
                use_doc_orientation_classify=True,
                use_doc_unwarping=False,
                det_limit_side_len=960,
                det_limit_type='max',
                use_angle_cls=True,
                show_log=False
            )
        
        return await loop.run_in_executor(self.executor, init_sync)
    
    async def process_document(self, file_path: str) -> str:
        """处理文档并返回OCR文本"""
        try:
            # 验证文件
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"文件不存在: {file_path}")
            
            # 执行OCR
            loop = asyncio.get_event_loop()
            ocr_result = await loop.run_in_executor(
                self.executor, 
                self._ocr_process_sync, 
                file_path
            )
            
            # 过滤和清理结果
            cleaned_text = self._clean_ocr_result(ocr_result)
            
            return cleaned_text
            
        except Exception as e:
            logger.error(f"OCR处理失败: {str(e)}", exc_info=True)
            raise
    
    def _ocr_process_sync(self, file_path: str) -> List[Dict[str, Any]]:
        """同步执行OCR"""
        result = self.ocr_engine.predict(input=file_path)
        
        # 提取文本和置信度
        watermarks = ['no', 'noi', 'copy', '样本']
        threshold = 0.5
        filtered_results = []
        
        for page_result in result:
            texts = page_result.get("rec_texts", [])
            scores = page_result.get("rec_scores", [])
            
            for text, score in zip(texts, scores):
                if score >= threshold and text.strip() and not any(wm in text.lower() for wm in watermarks):
                    filtered_results.append({
                        "text": text.strip(),
                        "confidence": float(score)
                    })
        
        return filtered_results
    
    def _clean_ocr_result(self, ocr_result: List[Dict[str, Any]]) -> str:
        """清理OCR结果"""
        if not ocr_result:
            return ""
        
        # 按置信度排序
        sorted_results = sorted(ocr_result, key=lambda x: x["confidence"], reverse=True)
        
        # 提取文本
        texts = [item["text"] for item in sorted_results]
        
        # 合并文本
        cleaned_text = "\n".join(texts)
        
        # 移除多余的空格和换行
        cleaned_text = "\n".join(line.strip() for line in cleaned_text.split("\n") if line.strip())
        
        return cleaned_text
    
    async def batch_process(self, file_paths: List[str]) -> Dict[str, str]:
        """批量处理文档"""
        tasks = [self.process_document(path) for path in file_paths]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        output = {}
        for path, result in zip(file_paths, results):
            if isinstance(result, Exception):
                output[path] = f"Error: {str(result)}"
            else:
                output[path] = result
        
        return output
    
    def get_supported_formats(self) -> List[str]:
        """获取支持的格式"""
        return ['pdf', 'png', 'jpg', 'jpeg', 'tiff', 'bmp']
    
    async def close(self):
        """关闭服务"""
        if self.executor:
            self.executor.shutdown(wait=True)
```

### 3.4 API接口设计

#### 3.4.1 文档处理接口

```python
# api/routes/documents.py
from fastapi import APIRouter, UploadFile, File, Form, HTTPException, BackgroundTasks, Depends
from fastapi.responses import JSONResponse, FileResponse
from typing import Optional, List
import uuid
import asyncio
import os

from api.schemas import (
    DocumentUploadRequest,
    DocumentUploadResponse,
    DocumentProcessRequest,
    DocumentStatusResponse,
    ExtractionResultResponse,
    DocumentListResponse,
    DocumentListItem
)
from api.dependencies import get_current_user, get_document_service
from services.document_service import DocumentService
from utils.file_utils import save_upload_file, validate_file_type, generate_file_path

router = APIRouter()

@router.post("/upload", response_model=DocumentUploadResponse)
async def upload_document(
    file: UploadFile = File(...),
    user_id: Optional[str] = Form(None),
    metadata: Optional[str] = Form(None, description="JSON格式的元数据"),
    current_user: dict = Depends(get_current_user),
    document_service: DocumentService = Depends(get_document_service)
):
    """
    上传文档
    
    - **file**: 上传的文件（PDF或图像）
    - **user_id**: 用户ID（可选）
    - **metadata**: 额外的元数据（JSON字符串）
    """
    try:
        # 验证文件大小和类型
        await validate_file_type(file)
        
        # 生成唯一ID和文件路径
        document_id = str(uuid.uuid4())
        file_path = await generate_file_path(document_id, file.filename)
        
        # 保存文件
        file_size = await save_upload_file(file, file_path)
        
        # 创建文档记录
        document = await document_service.create_document(
            document_id=document_id,
            user_id=user_id or current_user.get("id"),
            file_name=file.filename,
            file_path=file_path,
            file_size=file_size,
            file_type=file.content_type,
            metadata=metadata
        )
        
        return DocumentUploadResponse(
            document_id=document_id,
            status="uploaded",
            message="文档上传成功",
            file_name=file.filename,
            file_size=file_size,
            created_at=document["created_at"]
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"上传失败: {str(e)}")

@router.post("/{document_id}/process")
async def process_document(
    document_id: str,
    background_tasks: BackgroundTasks,
    request: Optional[DocumentProcessRequest] = None,
    current_user: dict = Depends(get_current_user),
    document_service: DocumentService = Depends(get_document_service)
):
    """
    处理文档
    
    - **document_id**: 文档ID
    - **process_type**: 处理类型（full, ocr_only, extract_only）
    """
    try:
        # 验证文档所有权
        if not await document_service.verify_document_owner(document_id, current_user.get("id")):
            raise HTTPException(status_code=403, detail="无权限访问此文档")
        
        # 检查文档状态
        status = await document_service.get_document_status(document_id)
        if status in ["processing", "completed"]:
            raise HTTPException(
                status_code=400, 
                detail=f"文档当前状态为{status}，无法重新处理"
            )
        
        # 启动后台处理任务
        background_tasks.add_task(
            document_service.process_document,
            document_id=document_id,
            user_id=current_user.get("id"),
            process_type=request.process_type if request else "full"
        )
        
        return {
            "document_id": document_id,
            "status": "processing",
            "message": "文档处理已开始",
            "estimated_time": "30-60秒"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"处理失败: {str(e)}")

@router.get("/{document_id}/status", response_model=DocumentStatusResponse)
async def get_document_status(
    document_id: str,
    current_user: dict = Depends(get_current_user),
    document_service: DocumentService = Depends(get_document_service)
):
    """获取文档处理状态"""
    try:
        # 验证文档所有权
        if not await document_service.verify_document_owner(document_id, current_user.get("id")):
            raise HTTPException(status_code=403, detail="无权限访问此文档")
        
        status_info = await document_service.get_document_status_info(document_id)
        
        return DocumentStatusResponse(
            document_id=document_id,
            status=status_info["status"],
            document_type=status_info.get("document_type"),
            progress=status_info.get("progress", 0),
            error_message=status_info.get("error_message"),
            processing_start=status_info.get("processing_start_time"),
            processing_end=status_info.get("processing_end_time"),
            created_at=status_info["created_at"],
            updated_at=status_info["updated_at"]
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"获取状态失败: {str(e)}")

@router.get("/{document_id}/result", response_model=ExtractionResultResponse)
async def get_extraction_result(
    document_id: str,
    current_user: dict = Depends(get_current_user),
    document_service: DocumentService = Depends(get_document_service)
):
    """获取提取结果"""
    try:
        # 验证文档所有权
        if not await document_service.verify_document_owner(document_id, current_user.get("id")):
            raise HTTPException(status_code=403, detail="无权限访问此文档")
        
        result = await document_service.get_extraction_result(document_id)
        
        if not result:
            raise HTTPException(status_code=404, detail="提取结果不存在")
        
        return ExtractionResultResponse(
            document_id=document_id,
            document_type=result["document_type"],
            extraction_data=result["extraction_data"],
            confidence_score=result["confidence_score"],
            processing_time=result["processing_time"],
            raw_ocr_text=result.get("raw_ocr_text"),
            created_at=result["created_at"]
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"获取结果失败: {str(e)}")

@router.get("/{document_id}/download")
async def download_document(
    document_id: str,
    current_user: dict = Depends(get_current_user),
    document_service: DocumentService = Depends(get_document_service)
):
    """下载原始文档"""
    try:
        # 验证文档所有权
        if not await document_service.verify_document_owner(document_id, current_user.get("id")):
            raise HTTPException(status_code=403, detail="无权限访问此文档")
        
        document = await document_service.get_document(document_id)
        if not document:
            raise HTTPException(status_code=404, detail="文档不存在")
        
        file_path = document["file_path"]
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail="文件不存在")
        
        return FileResponse(
            path=file_path,
            filename=document["file_name"],
            media_type=document["file_type"]
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"下载失败: {str(e)}")

@router.get("/", response_model=DocumentListResponse)
async def list_documents(
    page: int = 1,
    limit: int = 20,
    status: Optional[str] = None,
    document_type: Optional[str] = None,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    current_user: dict = Depends(get_current_user),
    document_service: DocumentService = Depends(get_document_service)
):
    """列出文档"""
    try:
        documents = await document_service.list_documents(
            user_id=current_user.get("id"),
            page=page,
            limit=limit,
            status=status,
            document_type=document_type,
            start_date=start_date,
            end_date=end_date
        )
        
        total = await document_service.count_documents(
            user_id=current_user.get("id"),
            status=status,
            document_type=document_type,
            start_date=start_date,
            end_date=end_date
        )
        
        items = [
            DocumentListItem(
                document_id=doc["id"],
                file_name=doc["file_name"],
                file_size=doc["file_size"],
                status=doc["status"],
                document_type=doc.get("document_type"),
                created_at=doc["created_at"],
                updated_at=doc["updated_at"]
            )
            for doc in documents
        ]
        
        return DocumentListResponse(
            items=items,
            total=total,
            page=page,
            limit=limit,
            has_more=(page * limit) < total
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"查询失败: {str(e)}")

@router.delete("/{document_id}")
async def delete_document(
    document_id: str,
    current_user: dict = Depends(get_current_user),
    document_service: DocumentService = Depends(get_document_service)
):
    """删除文档"""
    try:
        # 验证文档所有权
        if not await document_service.verify_document_owner(document_id, current_user.get("id")):
            raise HTTPException(status_code=403, detail="无权限访问此文档")
        
        success = await document_service.delete_document(document_id)
        
        if not success:
            raise HTTPException(status_code=500, detail="删除失败")
        
        return {
            "document_id": document_id,
            "message": "文档删除成功"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"删除失败: {str(e)}")
```

### 3.5 部署配置

#### 3.5.1 Docker部署

```dockerfile
# Dockerfile
# 使用Python 3.11作为基础镜像
FROM python:3.11-slim

# 设置工作目录
WORKDIR /app

# 设置环境变量
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV PIP_NO_CACHE_DIR=1
ENV PIP_DISABLE_PIP_VERSION_CHECK=1

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    libgl1-mesa-glx \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# 复制依赖文件
COPY requirements.txt .

# 安装Python依赖
RUN pip install --upgrade pip && \
    pip install -r requirements.txt --no-cache-dir

# 复制应用代码
COPY . .

# 创建必要的目录
RUN mkdir -p uploads logs models

# 设置权限
RUN chmod +x scripts/*.sh

# 暴露端口
EXPOSE 8000

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/api/health')" || exit 1

# 启动命令
CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### 3.5.2 Docker Compose配置

```yaml
# docker-compose.yml
version: '3.8'

services:
  api:
    build: .
    container_name: ocr-api
    ports:
      - "8000:8000"
    volumes:
      - ./uploads:/app/uploads
      - ./models:/app/models
      - ./logs:/app/logs
      - ./config:/app/config
    environment:
      - DEBUG=${DEBUG:-false}
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_KEY=${SUPABASE_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REDIS_URL=redis://redis:6379/0
    env_file:
      - .env
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - ocr-network

  redis:
    image: redis:7-alpine
    container_name: ocr-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    restart: unless-stopped
    networks:
      - ocr-network

  nginx:
    image: nginx:alpine
    container_name: ocr-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - ./logs/nginx:/var/log/nginx
    depends_on:
      - api
    restart: unless-stopped
    networks:
      - ocr-network

networks:
  ocr-network:
    driver: bridge

volumes:
  redis-data:
  uploads:
  models:
```

#### 3.5.3 Nginx配置

```nginx
# nginx/nginx.conf
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 1024;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;
    
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';
    
    access_log /var/log/nginx/access.log main;
    
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    
    # 文件上传大小限制
    client_max_body_size 20M;
    
    # Gzip压缩
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types text/plain text/css text/xml text/javascript 
               application/json application/javascript application/xml+rss 
               application/xml application/x-httpd-php image/svg+xml;
    
    # 上游服务
    upstream ocr_api {
        server api:8000;
    }
    
    # HTTP服务器（重定向到HTTPS）
    server {
        listen 80;
        server_name your-domain.com;
        return 301 https://$server_name$request_uri;
    }
    
    # HTTPS服务器
    server {
        listen 443 ssl http2;
        server_name your-domain.com;
        
        ssl_certificate /etc/nginx/ssl/your-domain.crt;
        ssl_certificate_key /etc/nginx/ssl/your-domain.key;
        
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers HIGH:!aNULL:!MD5;
        ssl_prefer_server_ciphers on;
        
        # API路由
        location /api/ {
            proxy_pass http://ocr_api;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # 超时设置
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
            
            # WebSocket支持
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
        }
        
        # 静态文件
        location /uploads/ {
            alias /app/uploads/;
            expires 7d;
            add_header Cache-Control "public, immutable";
        }
        
        # 健康检查
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
        
        # 根目录
        location / {
            root /usr/share/nginx/html;
            index index.html;
            try_files $uri $uri/ /index.html;
        }
    }
}
```

### 3.6 测试脚本

#### 3.6.1 API测试脚本

```python
# tests/test_api.py
import pytest
import asyncio
import json
from fastapi.testclient import TestClient
from api.main import app

client = TestClient(app)

def test_health_check():
    """测试健康检查"""
    response = client.get("/api/health")
    assert response.status_code == 200
    assert response.json()["status"] == "healthy"

def test_upload_document():
    """测试文档上传"""
    # 创建一个测试文件
    test_file = ("test.pdf", b"test content", "application/pdf")
    
    response = client.post(
        "/api/documents/upload",
        files={"file": test_file},
        data={"user_id": "test_user"}
    )
    
    assert response.status_code == 200
    data = response.json()
    assert "document_id" in data
    assert data["status"] == "uploaded"
    
    return data["document_id"]

def test_process_document():
    """测试文档处理"""
    # 先上传文档
    document_id = test_upload_document()
    
    response = client.post(f"/api/documents/{document_id}/process")
    assert response.status_code == 200
    assert response.json()["status"] == "processing"

def test_get_document_status():
    """测试获取文档状态"""
    document_id = test_upload_document()
    
    response = client.get(f"/api/documents/{document_id}/status")
    assert response.status_code == 200
    assert "status" in response.json()

def test_list_documents():
    """测试列出文档"""
    response = client.get("/api/documents/", params={"page": 1, "limit": 10})
    assert response.status_code == 200
    assert "items" in response.json()
    assert "total" in response.json()

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

#### 3.6.2 OCR测试脚本

```python
# tests/test_ocr.py
import asyncio
import os
from services.ocr_service import OCRService

async def test_ocr_processing():
    """测试OCR处理"""
    ocr_service = OCRService()
    await ocr_service.initialize()
    
    # 测试文件路径
    test_file = "tests/test_documents/sample_invoice.pdf"
    
    if not os.path.exists(test_file):
        print(f"测试文件不存在: {test_file}")
        return
    
    try:
        # 执行OCR
        print("开始OCR处理...")
        result = await ocr_service.process_document(test_file)
        
        print(f"OCR结果长度: {len(result)} 字符")
        print("前500字符:")
        print(result[:500])
        
        # 检查置信度
        confidence = ocr_service._calculate_confidence([{"confidence": 0.9}])
        print(f"置信度计算: {confidence}")
        
        # 批量处理测试
        test_files = [test_file, test_file]
        batch_results = await ocr_service.batch_process(test_files)
        print(f"批量处理完成: {len(batch_results)} 个文件")
        
    except Exception as e:
        print(f"OCR测试失败: {e}")
    
    finally:
        await ocr_service.close()

if __name__ == "__main__":
    asyncio.run(test_ocr_processing())
```

### 3.7 监控和日志

#### 3.7.1 日志配置

```python
# utils/logger.py
import logging
import sys
from logging.handlers import RotatingFileHandler
from datetime import datetime
import json

class JSONFormatter(logging.Formatter):
    """JSON格式日志格式化器"""
    
    def format(self, record):
        log_record = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
            "thread": record.threadName,
        }
        
        if record.exc_info:
            log_record["exception"] = self.formatException(record.exc_info)
        
        # 添加额外字段
        if hasattr(record, 'extra'):
            log_record.update(record.extra)
        
        return json.dumps(log_record, ensure_ascii=False)

def setup_logging(log_level="INFO", log_file="logs/app.log"):
    """配置日志"""
    # 创建日志目录
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    
    # 获取根日志记录器
    logger = logging.getLogger()
    logger.setLevel(getattr(logging, log_level.upper()))
    
    # 清除现有的处理器
    logger.handlers.clear()
    
    # 控制台处理器
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)
    
    # 文件处理器（JSON格式）
    file_handler = RotatingFileHandler(
        log_file,
        maxBytes=10*1024*1024,  # 10MB
        backupCount=10
    )
    file_handler.setLevel(logging.DEBUG)
    json_formatter = JSONFormatter()
    file_handler.setFormatter(json_formatter)
    logger.addHandler(file_handler)
    
    # 错误处理器
    error_handler = logging.FileHandler("logs/error.log")
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(json_formatter)
    logger.addHandler(error_handler)
    
    return logger

# 使用示例
logger = setup_logging()
```

#### 3.7.2 性能监控

```python
# utils/monitor.py
import time
import psutil
from typing import Dict, Any
from datetime import datetime

class PerformanceMonitor:
    """性能监控器"""
    
    def __init__(self):
        self.metrics = {}
        self.start_time = time.time()
    
    def start_operation(self, operation_name: str):
        """开始记录操作"""
        self.metrics[operation_name] = {
            "start_time": time.time(),
            "end_time": None,
            "duration": None,
            "memory_usage": psutil.Process().memory_info().rss / 1024 / 1024  # MB
        }
    
    def end_operation(self, operation_name: str):
        """结束记录操作"""
        if operation_name in self.metrics:
            end_time = time.time()
            self.metrics[operation_name]["end_time"] = end_time
            self.metrics[operation_name]["duration"] = (
                end_time - self.metrics[operation_name]["start_time"]
            )
    
    def get_metrics(self) -> Dict[str, Any]:
        """获取所有指标"""
        current_time = time.time()
        uptime = current_time - self.start_time
        
        # 系统指标
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        return {
            "timestamp": datetime.now().isoformat(),
            "uptime_seconds": uptime,
            "system": {
                "cpu_percent": cpu_percent,
                "memory_total_mb": memory.total / 1024 / 1024,
                "memory_used_mb": memory.used / 1024 / 1024,
                "memory_percent": memory.percent,
                "disk_total_gb": disk.total / 1024 / 1024 / 1024,
                "disk_used_gb": disk.used / 1024 / 1024 / 1024,
                "disk_percent": disk.percent
            },
            "operations": self.metrics
        }
    
    def reset(self):
        """重置监控器"""
        self.metrics = {}
        self.start_time = time.time()

# 使用装饰器监控函数性能
def monitor_performance(operation_name):
    """性能监控装饰器"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            monitor = PerformanceMonitor()
            monitor.start_operation(operation_name)
            
            try:
                result = func(*args, **kwargs)
                monitor.end_operation(operation_name)
                return result
            except Exception as e:
                monitor.end_operation(operation_name)
                raise e
            finally:
                # 记录指标
                metrics = monitor.get_metrics()
                logger.info(f"性能指标 - {operation_name}: {metrics}")
        
        return wrapper
    return decorator
```

## 四、实施计划

### 4.1 阶段一：基础环境搭建（1-2天）
1. 项目初始化与Git仓库设置
2. Python虚拟环境配置
3. 依赖包安装
4. Supabase项目创建与数据库初始化
5. 基础目录结构创建

### 4.2 阶段二：核心模块开发（3-5天）
1. 配置系统实现
2. 数据库模型定义
3. Supabase客户端封装
4. OCR服务封装
5. 基础FastAPI应用框架

### 4.3 阶段三：智能体工作流开发（3-4天）
1. LangGraph工作流定义
2. 文档分类智能体
3. 信息提取智能体
4. 状态管理与错误处理
5. 缓存与性能优化

### 4.4 阶段四：API接口开发（2-3天）
1. 文档上传接口
2. 文档处理接口
3. 状态查询接口
4. 结果获取接口
5. 用户认证与授权

### 4.5 阶段五：部署与测试（2-3天）
1. Docker容器化配置
2. Nginx反向代理配置
3. 单元测试编写
4. 集成测试
5. 性能测试与优化

### 4.6 阶段六：监控与文档（1-2天）
1. 日志系统集成
2. 性能监控
3. API文档生成
4. 用户使用文档
5. 部署文档

## 五、注意事项

### 5.1 安全性考虑
1. **文件上传安全**：验证文件类型、大小，扫描恶意内容
2. **API安全**：使用JWT认证、API限流、CORS配置
3. **数据安全**：敏感信息加密存储，访问控制
4. **环境安全**：密钥管理，最小权限原则

### 5.2 性能优化
1. **异步处理**：使用async/await避免阻塞
2. **缓存策略**：Redis缓存常用数据和模型
3. **批处理**：支持批量文档处理
4. **资源管理**：合理配置线程池和连接池

### 5.3 可扩展性
1. **模块化设计**：各组件松散耦合
2. **插件架构**：支持新的文档类型和提取规则
3. **配置驱动**：通过配置文件调整参数
4. **水平扩展**：支持多实例部署

### 5.4 错误处理
1. **重试机制**：对可恢复错误自动重试
2. **降级策略**：关键功能失败时提供基本服务
3. **监控告警**：错误日志和告警通知
4. **用户反馈**：清晰的错误信息和解决方案

## 六、维护与升级

### 6.1 日常维护
1. 监控系统运行状态
2. 定期备份数据
3. 日志分析与问题排查
4. 性能调优

### 6.2 版本升级
1. 保持依赖包更新
2. 定期安全审计
3. 向后兼容性测试
4. 灰度发布策略

### 6.3 功能扩展
1. 支持更多文档类型
2. 增加更多提取字段
3. 集成更多OCR引擎
4. 增加AI模型微调功能

## 七、预算与资源

### 7.1 硬件资源
- 服务器：2核4GB内存，50GB存储（推荐）
- 带宽：100Mbps（根据用户量调整）

### 7.2 软件资源
- Supabase：免费计划（10GB数据库，500MB文件存储）
- OpenAI API：按使用量计费
- 域名与SSL证书

### 7.3 人力资源
- 开发工程师：1-2人 × 2周
- 测试工程师：1人 × 1周
- 运维工程师：0.5人（持续）

## 八、风险与应对

### 8.1 技术风险
1. **OCR精度不足**：准备备用OCR引擎，人工复核机制
2. **LLM API不稳定**：本地模型备用，缓存策略
3. **并发处理瓶颈**：队列系统，水平扩展

### 8.2 业务风险
1. **用户量增长过快**：自动扩容机制，性能监控
2. **文档格式变化**：灵活的提取规则，模型定期更新
3. **法规合规要求**：数据加密，访问日志，审计功能

### 8.3 运维风险
1. **系统宕机**：高可用部署，自动恢复机制
2. **数据丢失**：定期备份，多点冗余
3. **安全漏洞**：定期安全扫描，及时打补丁

